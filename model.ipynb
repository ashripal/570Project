{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQsA4q-KughS",
        "outputId": "f3073b5d-7602-4f07-a770-57e8a408ec61"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.66.6)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "mv: cannot stat '/root/.cache/kagglehub/datasets/vishnutheepb/msrvtt/versions/1/*': No such file or directory\n",
            "Dataset files path: /content/msrvtt\n",
            "Dataset structure: Dataset({\n",
            "    features: ['video_id', 'caption', 'sen_id', 'category', 'url', 'start time', 'end time', 'split', 'id', '__index_level_0__'],\n",
            "    num_rows: 6513\n",
            "})\n",
            "First example: {'video_id': 'video0', 'caption': 'a car is shown', 'sen_id': 77300, 'category': 9, 'url': 'https://www.youtube.com/watch?v=9lZi22qLlEo', 'start time': 137.72, 'end time': 149.44, 'split': 'train', 'id': 0, '__index_level_0__': 0}\n",
            "Captions saved to: /content/msrvtt_captions.json\n",
            "Total videos found in captions file: 6513\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-3-8c52449db73e>:170: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Batch [64/1629], Loss: 0.12935574352741241\n",
            "Epoch [1/3], Batch [128/1629], Loss: 0.11462084203958511\n",
            "Epoch [1/3], Batch [192/1629], Loss: 0.12598364055156708\n",
            "Epoch [1/3], Batch [256/1629], Loss: 0.1266632080078125\n",
            "Epoch [1/3], Batch [320/1629], Loss: 0.11484319716691971\n",
            "Epoch [1/3], Batch [384/1629], Loss: 0.10541076958179474\n",
            "Epoch [1/3], Batch [448/1629], Loss: 0.11478233337402344\n",
            "Epoch [1/3], Batch [512/1629], Loss: 0.08833789825439453\n",
            "Epoch [1/3], Batch [576/1629], Loss: 0.10030937194824219\n",
            "Epoch [1/3], Batch [640/1629], Loss: 0.09641341865062714\n",
            "Epoch [1/3], Batch [704/1629], Loss: 0.09109151363372803\n",
            "Epoch [1/3], Batch [768/1629], Loss: 0.09230327606201172\n",
            "Epoch [1/3], Batch [832/1629], Loss: 0.08915825933218002\n",
            "Epoch [1/3], Batch [896/1629], Loss: 0.10894598066806793\n",
            "Epoch [1/3], Batch [960/1629], Loss: 0.09387648850679398\n",
            "Epoch [1/3], Batch [1024/1629], Loss: 0.0773005336523056\n",
            "Epoch [1/3], Batch [1088/1629], Loss: 0.11694017797708511\n",
            "Epoch [1/3], Batch [1152/1629], Loss: 0.10324530303478241\n",
            "Epoch [1/3], Batch [1216/1629], Loss: 0.09316810965538025\n",
            "Epoch [1/3], Batch [1280/1629], Loss: 0.10517338663339615\n",
            "Epoch [1/3], Batch [1344/1629], Loss: 0.10633765161037445\n",
            "Epoch [1/3], Batch [1408/1629], Loss: 0.07922210544347763\n",
            "Epoch [1/3], Batch [1472/1629], Loss: 0.10453612357378006\n",
            "Epoch [1/3], Batch [1536/1629], Loss: 0.09450880438089371\n",
            "Epoch [1/3], Batch [1600/1629], Loss: 0.09408990293741226\n",
            "Model saved for epoch 1\n",
            "Epoch 1 complete.\n",
            "Epoch [2/3], Batch [64/1629], Loss: 0.08547914028167725\n",
            "Epoch [2/3], Batch [128/1629], Loss: 0.09198431670665741\n",
            "Epoch [2/3], Batch [192/1629], Loss: 0.10049983114004135\n",
            "Epoch [2/3], Batch [256/1629], Loss: 0.11161363869905472\n",
            "Epoch [2/3], Batch [320/1629], Loss: 0.09332690387964249\n",
            "Epoch [2/3], Batch [384/1629], Loss: 0.08709892630577087\n",
            "Epoch [2/3], Batch [448/1629], Loss: 0.07527473568916321\n",
            "Epoch [2/3], Batch [512/1629], Loss: 0.09181112051010132\n",
            "Epoch [2/3], Batch [576/1629], Loss: 0.0836082249879837\n",
            "Epoch [2/3], Batch [640/1629], Loss: 0.09745398163795471\n",
            "Epoch [2/3], Batch [704/1629], Loss: 0.09224068373441696\n",
            "Epoch [2/3], Batch [768/1629], Loss: 0.08755989372730255\n",
            "Epoch [2/3], Batch [832/1629], Loss: 0.10208520293235779\n",
            "Epoch [2/3], Batch [896/1629], Loss: 0.09999799728393555\n",
            "Epoch [2/3], Batch [960/1629], Loss: 0.08848898857831955\n",
            "Epoch [2/3], Batch [1024/1629], Loss: 0.1028209701180458\n",
            "Epoch [2/3], Batch [1088/1629], Loss: 0.09849853813648224\n",
            "Epoch [2/3], Batch [1152/1629], Loss: 0.10501670837402344\n",
            "Epoch [2/3], Batch [1216/1629], Loss: 0.07606919854879379\n",
            "Epoch [2/3], Batch [1280/1629], Loss: 0.11038284003734589\n",
            "Epoch [2/3], Batch [1344/1629], Loss: 0.0810195729136467\n",
            "Epoch [2/3], Batch [1408/1629], Loss: 0.10016928613185883\n",
            "Epoch [2/3], Batch [1472/1629], Loss: 0.07945377379655838\n",
            "Epoch [2/3], Batch [1536/1629], Loss: 0.09937906265258789\n",
            "Epoch [2/3], Batch [1600/1629], Loss: 0.0865398421883583\n",
            "Model saved for epoch 2\n",
            "Epoch 2 complete.\n",
            "Epoch [3/3], Batch [64/1629], Loss: 0.08389865607023239\n",
            "Epoch [3/3], Batch [128/1629], Loss: 0.07452650368213654\n",
            "Epoch [3/3], Batch [192/1629], Loss: 0.08628884702920914\n",
            "Epoch [3/3], Batch [256/1629], Loss: 0.08883019536733627\n",
            "Epoch [3/3], Batch [320/1629], Loss: 0.09726188331842422\n",
            "Epoch [3/3], Batch [384/1629], Loss: 0.09145147353410721\n",
            "Epoch [3/3], Batch [448/1629], Loss: 0.06853973120450974\n",
            "Epoch [3/3], Batch [512/1629], Loss: 0.10447127372026443\n",
            "Epoch [3/3], Batch [576/1629], Loss: 0.09793128818273544\n",
            "Epoch [3/3], Batch [640/1629], Loss: 0.08791644871234894\n",
            "Epoch [3/3], Batch [704/1629], Loss: 0.097264863550663\n",
            "Epoch [3/3], Batch [768/1629], Loss: 0.08422338217496872\n",
            "Epoch [3/3], Batch [832/1629], Loss: 0.0868067741394043\n",
            "Epoch [3/3], Batch [896/1629], Loss: 0.10106252133846283\n",
            "Epoch [3/3], Batch [960/1629], Loss: 0.08793192356824875\n",
            "Epoch [3/3], Batch [1024/1629], Loss: 0.07539688795804977\n",
            "Epoch [3/3], Batch [1088/1629], Loss: 0.08936037123203278\n",
            "Epoch [3/3], Batch [1152/1629], Loss: 0.08281917870044708\n",
            "Epoch [3/3], Batch [1216/1629], Loss: 0.08689238876104355\n",
            "Epoch [3/3], Batch [1280/1629], Loss: 0.08078119158744812\n",
            "Epoch [3/3], Batch [1344/1629], Loss: 0.06609094142913818\n",
            "Epoch [3/3], Batch [1408/1629], Loss: 0.08270810544490814\n",
            "Epoch [3/3], Batch [1472/1629], Loss: 0.08955764770507812\n",
            "Epoch [3/3], Batch [1536/1629], Loss: 0.10115129500627518\n",
            "Epoch [3/3], Batch [1600/1629], Loss: 0.08688163757324219\n",
            "Model saved for epoch 3\n",
            "Epoch 3 complete.\n",
            "Model saved.\n",
            "\n",
            "Testing Generated Captions Against Original Captions:\n",
            "\n",
            "Video ID: video0\n",
            "Original Caption: ('a demonstration of someone playing minecraft', 'a man in a black suit discusses election with a man appearing on a tv screen', '3 kids singing on the voice', 'a cartoon of a spider and the sun')\n",
            "Generated Caption: Video summary:\n",
            "\n",
            "This is a game about a group of people who are trying to save the world from a group that is trying to destroy it.\n",
            "\n",
            "Video ID: video1\n",
            "Original Caption: ('ted cruz speaks with a white haired man during the presidential debate', 'a bunch of people are doing a popular dance', '3d african animals running', 'a girl is talking and putting on a hat in her room')\n",
            "Generated Caption: Video summary:\n",
            "\n",
            "A man is killed and a woman is injured in a car crash on the outskirts of the capital on Sunday morning.\n",
            "\n",
            "Police say a man is shot in the leg and is taken to hospital with a gunshot wound to the leg.\n",
            "\n",
            "Video ID: video10\n",
            "Original Caption: ('two chef talking about how lobster is considered very glamorous', 'a comedian tells a story on stage', 'a car that is being sold', '3 men are talking in a room')\n",
            "Generated Caption: Video summary:\n",
            "\n",
            "This is a game about a group of people trying to figure out how to survive in a world that is filled with monsters and monsters of various shapes and sizes. Players take on the role of a young boy and his group of friends trying to save the world from a group that is trying to destroy it.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install --no-cache-dir kagglehub kaggle datasets transformers\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ViTModel, GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import cv2\n",
        "from datasets import load_dataset\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Set up and download MSR-VTT dataset\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"vishnutheepb/msrvtt\")\n",
        "dest_path = \"/content/msrvtt\"\n",
        "\n",
        "!mkdir -p $dest_path\n",
        "!mv $path/* $dest_path\n",
        "print(\"Dataset files path:\", dest_path)\n",
        "\n",
        "# ===========================\n",
        "# Step 1: Load MSR-VTT Dataset\n",
        "# ===========================\n",
        "dataset = load_dataset(\"AlexZigma/msr-vtt\", split=\"train\")\n",
        "print(\"Dataset structure:\", dataset)\n",
        "print(\"First example:\", dataset[0])\n",
        "\n",
        "# Save video captions to JSON file\n",
        "captions_dict = {example[\"video_id\"]: example[\"caption\"] for example in dataset}\n",
        "captions_path = \"/content/msrvtt_captions.json\"\n",
        "with open(captions_path, \"w\") as f:\n",
        "    json.dump(captions_dict, f)\n",
        "print(\"Captions saved to:\", captions_path)\n",
        "\n",
        "# Set video directory\n",
        "video_dir = \"/content/msrvtt/TrainValVideo\"\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "# ===========================\n",
        "# Step 2: Define DataLoader\n",
        "# ===========================\n",
        "class MSRVTTDataset(Dataset):\n",
        "    def __init__(self, video_dir, captions_path, frame_size=(224, 224), frames_per_clip=16):\n",
        "        self.video_dir = video_dir\n",
        "        self.frame_size = frame_size\n",
        "        self.frames_per_clip = frames_per_clip\n",
        "        with open(captions_path, 'r') as f:\n",
        "            self.captions_data = json.load(f)\n",
        "        self.video_files = list(self.captions_data.keys())\n",
        "        print(f\"Total videos found in captions file: {len(self.video_files)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_files)\n",
        "\n",
        "    # Updated to skip files not locally available\n",
        "    def load_video_frames(self, video_path):\n",
        "        if not os.path.exists(video_path):\n",
        "            print(f\"Skipping video file not found : {video_path}\")\n",
        "            return None\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "            frame = cv2.resize(frame, self.frame_size)\n",
        "            frame = torch.tensor(frame).permute(2, 0, 1) / 255.0\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "\n",
        "        if len(frames) == 0:\n",
        "          print(f\"Warning: No frames read from video file {video_path}.\")\n",
        "          return None\n",
        "        frames = frames[:self.frames_per_clip]\n",
        "        return torch.stack(frames) if len(frames) > 0 else None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        while True:\n",
        "            video_file = self.video_files[idx]\n",
        "            video_path = os.path.join(self.video_dir, f\"{video_file}.mp4\")\n",
        "            frames = self.load_video_frames(video_path)\n",
        "            if frames is not None:\n",
        "                caption = self.captions_data[video_file]\n",
        "                return frames, caption\n",
        "            else:\n",
        "                print(f\"Skipping index {idx} due to missing frames.\")\n",
        "                idx = (idx + 1) % len(self)\n",
        "\n",
        "dataset = MSRVTTDataset(video_dir, captions_path)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# ===========================\n",
        "# Step 3: Model Definition\n",
        "# ===========================\n",
        "resnet50 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "resnet_feature_dim = resnet50.fc.in_features\n",
        "resnet50.fc = nn.Identity()  # Remove final layer\n",
        "resnet50 = resnet50.cuda()\n",
        "vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224\").cuda()\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos token\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").cuda()\n",
        "\n",
        "# combined model\n",
        "class ConViTCaptioning(nn.Module):\n",
        "    def __init__(self, resnet50, vit, gpt2_model, resnet_feature_dim, embed_dim=768):\n",
        "        super(ConViTCaptioning, self).__init__()\n",
        "        self.resnet = resnet50\n",
        "        self.vit = vit\n",
        "        self.gpt2 = gpt2_model\n",
        "        self.proj_layer = nn.Linear(resnet_feature_dim + vit.config.hidden_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, frames):\n",
        "        # Extract local features with ResNet\n",
        "        local_features = self.resnet(frames)  # Process batch of frames\n",
        "        local_features = local_features.view(frames.size(0), -1)  # Flatten if necessary\n",
        "\n",
        "        # Extract global features with ViT\n",
        "        vit_features = self.vit(pixel_values=frames).pooler_output  # Shape: (batch_size, vit_hidden_size)\n",
        "\n",
        "        # Combine features and project to GPT-2 embedding size\n",
        "        combined_features = torch.cat((local_features, vit_features), dim=1)  # Shape: (batch_size, resnet_feat_dim + vit_hidden_size)\n",
        "        projected_features = self.proj_layer(combined_features)  # Shape: (batch_size, embed_dim)\n",
        "        projected_features = self.dropout(projected_features)  # Apply dropout here\n",
        "\n",
        "        return projected_features\n",
        "\n",
        "model = ConViTCaptioning(resnet50, vit, gpt2_model, resnet_feature_dim).cuda()\n",
        "optimizer = torch.optim.Adam(gpt2_model.parameters(), lr=1e-4)\n",
        "scaler = GradScaler()\n",
        "\n",
        "# ===========================\n",
        "# Step 4: Reward Model Definition + Caption Generation\n",
        "# ===========================\n",
        "def reward_function(generated_caption, reference_caption):\n",
        "    generated_tokens = set(generated_caption.lower().split())\n",
        "    reference_tokens = set(reference_caption[0].lower().split())\n",
        "    overlap = len(generated_tokens.intersection(reference_tokens))\n",
        "    return overlap / len(reference_tokens)\n",
        "\n",
        "def generate_caption(features, reference_caption, threshold=0.7, max_attempts=5):\n",
        "    best_caption = \"\"\n",
        "    best_reward = 0\n",
        "    for attempt in range(max_attempts):\n",
        "        input_ids = tokenizer(\"Video summary:\", return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=20).input_ids.cuda()\n",
        "        attention_mask = (input_ids != tokenizer.pad_token_id).long().cuda()\n",
        "\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            output_ids = gpt2_model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_length=100,\n",
        "                num_return_sequences=1,\n",
        "                no_repeat_ngram_size=3,\n",
        "                top_k=50,\n",
        "                top_p=0.9,\n",
        "                temperature=0.6,\n",
        "                num_beams=7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "        generated_caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = reward_function(generated_caption, reference_caption)\n",
        "        if reward > best_reward:\n",
        "            best_caption = generated_caption\n",
        "            best_reward = reward\n",
        "        if reward >= threshold:\n",
        "            return generated_caption, reward\n",
        "    return best_caption, best_reward  # Return the best caption after attempts\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Step 5: Training Loop (Updated for Multi-Video Batch Processing and Cross-Entropy Loss)\n",
        "# ===========================\n",
        "num_training_steps = len(dataloader) * 3  # 3 epochs\n",
        "num_warmup_steps = int(0.1 * num_training_steps)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
        "epochs = 3\n",
        "batch_size = 4\n",
        "model = ConViTCaptioning(resnet50, vit, gpt2_model, resnet_feature_dim).cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "scaler = torch.amp.GradScaler()\n",
        "\n",
        "accumulation_steps = 64\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    valid_batches = 0\n",
        "    for batch_idx, (frames, captions) in enumerate(dataloader):\n",
        "        if frames is None or captions is None:\n",
        "            continue\n",
        "        frames = frames.cuda()\n",
        "        current_batch_size, num_frames, channels, height, width = frames.shape\n",
        "\n",
        "        # Reshape frames for batch processing\n",
        "        frames = frames.view(current_batch_size * num_frames, channels, height, width)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            # Forward pass to get projected features\n",
        "            projected_features = model(frames)\n",
        "            projected_features = projected_features.view(current_batch_size, num_frames, -1)\n",
        "            video_features = projected_features.mean(dim=1)  # Average over frames\n",
        "\n",
        "            # Prepare input and labels for GPT-2\n",
        "            inputs = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True).input_ids.cuda()\n",
        "            labels = inputs.clone()\n",
        "\n",
        "            # Generate output from GPT-2 based on projected features\n",
        "            outputs = gpt2_model(inputs_embeds=video_features.unsqueeze(1).repeat(1, inputs.shape[1], 1), labels=labels)\n",
        "            loss = outputs.loss / accumulation_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item()}\")\n",
        "    torch.save(model.state_dict(), f\"convit_captioning_model_epoch_{epoch+1}.pth\")\n",
        "    print(f\"Model saved for epoch {epoch+1}\")\n",
        "    print(f\"Epoch {epoch+1} complete.\")\n",
        "# ===========================\n",
        "# Step 6: Save Model\n",
        "# ===========================\n",
        "torch.save(model.state_dict(), \"convit_captioning_model.pth\")\n",
        "print(\"Model saved.\")\n",
        "\n",
        "print(\"\\nTesting Generated Captions Against Original Captions:\\n\")\n",
        "model.eval()\n",
        "test_count = 3\n",
        "with torch.no_grad():\n",
        "    for idx, (frames, captions) in enumerate(dataloader):\n",
        "        if idx >= test_count:\n",
        "            break\n",
        "        frames = frames.cuda()\n",
        "        batch_size, num_frames, channels, height, width = frames.shape\n",
        "        frames = frames.view(batch_size * num_frames, channels, height, width)\n",
        "        features = vit(pixel_values=frames)\n",
        "        features = features.last_hidden_state\n",
        "        features = features.view(batch_size, num_frames, -1)\n",
        "        video_features = features.mean(dim=1)\n",
        "        generated_caption, _ = generate_caption(video_features, captions)\n",
        "        print(f\"Video ID: {dataset.video_files[idx]}\")\n",
        "        print(f\"Original Caption: {captions}\")\n",
        "        print(f\"Generated Caption: {generated_caption}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), \"convit_captioning_model.pth\")\n",
        "# print(\"Model saved.\")\n",
        "\n",
        "print(\"\\nTesting Generated Captions Against Original Captions:\\n\")\n",
        "model.eval()\n",
        "test_count = 3\n",
        "with torch.no_grad():\n",
        "    for idx, (frames, captions) in enumerate(dataloader):\n",
        "        if idx >= test_count:\n",
        "            break\n",
        "        frames = frames.cuda()\n",
        "        batch_size, num_frames, channels, height, width = frames.shape\n",
        "        frames = frames.view(batch_size * num_frames, channels, height, width)\n",
        "        features = vit(pixel_values=frames)\n",
        "        features = features.last_hidden_state\n",
        "        features = features.view(batch_size, num_frames, -1)\n",
        "        video_features = features.mean(dim=1)\n",
        "        generated_caption, _ = generate_caption(video_features, captions)\n",
        "        print(f\"Video ID: {dataset.video_files[idx]}\")\n",
        "        print(f\"Original Caption: {captions}\")\n",
        "        print(f\"Generated Caption: {generated_caption}\\n\")"
      ],
      "metadata": {
        "id": "egmt-pdJTZsZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "475da8af-e150-4279-9a6a-b9708d36321c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Generated Captions Against Original Captions:\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-99053355bcd7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTesting Generated Captions Against Original Captions:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ]
}